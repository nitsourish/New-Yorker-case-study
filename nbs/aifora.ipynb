{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is consist of following regarding the Data Science Take Home Project from aifora\n",
    "\n",
    "- data exploration & preparation\n",
    "- Time series analysis\n",
    "- feature engineering and feature analysis\n",
    "- implementation of machine learning model to forecast the sales\n",
    "- model evaluation & inference\n",
    "- A breif analysis and iimplementation of price elasticity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sktime\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from sklearn.utils import shuffle \n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from fastai.tabular.core import add_datepart\n",
    "from sklearn import *\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, log_loss, make_scorer,\n",
    "                             roc_auc_score)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, ParameterGrid,\n",
    "                                     ParameterSampler, RandomizedSearchCV,\n",
    "                                     StratifiedKFold, StratifiedShuffleSplit,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.utils import resample\n",
    "from datetime import datetime, timedelta\n",
    "from sktime.forecasting.arima import ARIMA,AutoARIMA\n",
    "from sktime.forecasting.sarimax import SARIMAX\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.var import VAR\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.forecasting.fbprophet import Prophet\n",
    "from datetime import datetime, timedelta\n",
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.compose import make_reduction,MultioutputTimeSeriesRegressionForecaster,DirectTabularRegressionForecaster,MultioutputTabularRegressionForecaster\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split, SingleWindowSplitter,SlidingWindowSplitter\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from termcolor import colored\n",
    "from sktime.forecasting.model_selection import ForecastingRandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, ClassifierMixin\n",
    "from pandas_profiling import ProfileReport\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration & preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/transaction_grid.csv')\n",
    "print(train.shape)\n",
    "display(train.head())\n",
    "\n",
    "#no of style_color_group for each level3\n",
    "display(train[['level4','level3']].groupby(['level3']).agg(['nunique','unique']).reset_index())\n",
    "\n",
    "#no of style_color_group for each level3\n",
    "display(train[['style_color_group','level3']].groupby(['level3']).agg(['nunique','unique']).reset_index())\n",
    "\n",
    "#no of style_group for each level3\n",
    "display(train[['style_group','level3']].groupby(['level3']).agg(['nunique','unique']).reset_index())\n",
    "\n",
    "#no of style_color_group for each level3 & level4 combination\n",
    "display(train[['style_color_group','level3','level4']].groupby(['level3','level4']).agg(['nunique','unique']).reset_index())\n",
    "\n",
    "#no of style_group for each level3 & level4 combination\n",
    "display(train[['style_group','level3','level4']].groupby(['level3','level4']).agg(['nunique','unique']).reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of each style_color_group in the dataset \n",
    "train['transaction_date'] = pd.to_datetime(train['transaction_date'])\n",
    "display(train[['style_color_group','transaction_date']].groupby(['style_color_group']).agg(['count','min','max']).reset_index())\n",
    "\n",
    "#count of each style_group in the dataset \n",
    "train['transaction_date'] = pd.to_datetime(train['transaction_date'])\n",
    "train[['style_group','transaction_date']].groupby(['style_group']).agg(['count','min','max']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the data-size is not adequate to forecast in style_color_group will forecast in style_group level \n",
    "\n",
    "##### - Also we do not need original_price as feature and may use either 'sales_price' or 'final_price' as price feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "selected = ['sales_channel', 'units_sold',\n",
    "       'original_price', 'sales_price', 'final_price', 'is_promotion',\n",
    "       'month_of_launch', 'season_start_date', 'season_end_date',\n",
    "       'current_season', 'initial_season']\n",
    "prof = ProfileReport(train[selected], title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "prof.to_file(output_file='output.html')\n",
    "prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fortunately there is no missing value in the data\n",
    "- Naturally stong positive co-relation between units_sold and price especially with final_price & promotion\n",
    "- as sales_channel has only one value, we can drop it\n",
    "- As expected in real world, units_sold is left skewed with 21% of 0 sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation:  style_group level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(train:pd.DataFrame):\n",
    "\n",
    "    \"\"\"\n",
    "   style_group level Over all Data prep function for forecasting.\n",
    "\n",
    "    Args:\n",
    "        transaction Dataframe. \n",
    "    Returns:\n",
    "        style_group level aggregated sales and average price.\n",
    "    \"\"\"\n",
    "\n",
    "    train['transaction_date'] = pd.to_datetime(train['transaction_date'])\n",
    "\n",
    "    df = train[['level3', 'level4','style_group','transaction_date', 'units_sold','sales_price', 'final_price', \\\n",
    "                                'is_promotion']].groupby(['level3', 'level4','style_group','transaction_date']).\\\n",
    "                                agg({'sales_price':'mean', 'final_price':'mean','is_promotion':'mean','units_sold':'sum',\\\n",
    "                                                                                            'final_price':'mean'}).reset_index().fillna(0)\n",
    "    \n",
    "    return df                                                                                   \n",
    "\n",
    "df = data_prep(train)\n",
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Snek-peak different series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_style_group = df.style_group.unique()\n",
    "import matplotlib\n",
    "import pylab\n",
    "for i in unique_style_group:\n",
    "    train_1 = df[df.style_group == i][['transaction_date', 'units_sold']]\n",
    "    grpd_weekly = train_1.resample('W',on='transaction_date').sum()\n",
    "    grpd_weekly.plot(figsize = (10,3), title = f'style_group_code {i}',xlabel = 'Date',ylabel = 'count')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High Volatility and unpredictability coupled with sporadic demand\n",
    "- Seasonal fashion trends - shorter product life cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STL decomposition of a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "for style_group in unique_style_group[0:5]:\n",
    "    train_1 = df[df.style_group == style_group][['transaction_date', 'units_sold']]\n",
    "    grpd_weekly = train_1.resample('W',on='transaction_date').sum()\n",
    "    y = grpd_weekly.squeeze()\n",
    "    result = seasonal_decompose(y,model='additive',period=7)\n",
    "    display(f'style group {style_group}')\n",
    "    result.plot(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected visible seasonal component in sales.\n",
    "- Further we may understand the frequency of the seasonality by converting time signal to frequency domain using Fourier transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering- Time Sereies property based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonality analysis and FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True, figsize=(11, 5))\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# annotations: https://stackoverflow.com/a/49238256/5769929\n",
    "def seasonal_plot(X, y, period, freq, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n",
    "    ax = sns.lineplot(\n",
    "        x=freq,\n",
    "        y=y,\n",
    "        hue=period,\n",
    "        data=X,\n",
    "        ci=False,\n",
    "        ax=ax,\n",
    "        palette=palette,\n",
    "        legend=False,\n",
    "    )\n",
    "    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n",
    "    for line, name in zip(ax.lines, X[period].unique()):\n",
    "        y_ = line.get_ydata()[-1]\n",
    "        ax.annotate(\n",
    "            name,\n",
    "            xy=(1, y_),\n",
    "            xytext=(6, 0),\n",
    "            color=line.get_color(),\n",
    "            xycoords=ax.get_yaxis_transform(),\n",
    "            textcoords=\"offset points\",\n",
    "            size=14,\n",
    "            va=\"center\",\n",
    "        )\n",
    "    return ax\n",
    "\n",
    "#Seasonal Plot for each style_group\n",
    "for i in unique_style_group:\n",
    "    \n",
    "    train_1 = df[df.style_group == i][['transaction_date', 'units_sold']]\n",
    "    train_1 = train_1.set_index('transaction_date')\n",
    "    train_1[\"day\"] = train_1.index.dayofweek  # the x-axis (freq)\n",
    "    train_1[\"week\"] = train_1.index.week  # the seasonal period (period)\n",
    "\n",
    "    # days within a year\n",
    "    train_1[\"dayofyear\"] = train_1.index.dayofyear\n",
    "    train_1[\"year\"] = train_1.index.year\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    seasonal_plot(train_1, y=\"units_sold\", period=\"year\", freq=\"dayofyear\", ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### periodogram & seasonality selection\n",
    "\n",
    "- The periodogram tells us the strength of the frequencies in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_periodogram(ts, detrend='linear', ax=None):\n",
    "    from scipy.signal import periodogram\n",
    "    fs = pd.Timedelta(\"365D\") / pd.Timedelta(\"1D\")\n",
    "    freqencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend=detrend,\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.step(freqencies, spectrum, color=\"purple\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Annual (1)\",\n",
    "            \"Semiannual (2)\",\n",
    "            \"Quarterly (4)\",\n",
    "            \"Bimonthly (6)\",\n",
    "            \"Monthly (12)\",\n",
    "            \"Biweekly (26)\",\n",
    "            \"Weekly (52)\",\n",
    "            \"Semiweekly (104)\",\n",
    "        ],\n",
    "        rotation=30,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Periodogram\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "for i in unique_style_group:\n",
    "    train_1 = df[df.style_group == i][['transaction_date', 'units_sold']]\n",
    "    plot_periodogram(train_1.units_sold, detrend='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostly quarterly seasonality(frequency = 4)\n",
    "\n",
    "- From right to left, the periodogram falls off between Bimonthly (6) and quarterly (12), so let's use 4 Fourier pairs of frequency 4\n",
    "- So it will create 8 new features based on the seasonality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal feature generator\n",
    "\n",
    "def fourier_features(index, freq, order):\n",
    "\n",
    "    '''create fourier features for a given frequency and order '''\n",
    "    \n",
    "    time = np.arange(len(index), dtype=np.float32)\n",
    "    k = 2 * np.pi * (1 / freq) * time\n",
    "    features = {}\n",
    "    for i in range(1, order + 1):\n",
    "        features.update({\n",
    "            f\"sin_{freq}_{i}\": np.sin(i * k),\n",
    "            f\"cos_{freq}_{i}\": np.cos(i * k),\n",
    "        })\n",
    "    return pd.DataFrame(features, index=index)\n",
    "train_1 = df[df.style_group == style_group][['transaction_date', 'units_sold']]\n",
    "fourier_features(train_1.units_sold, freq=4, order=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "#### We will generate following features for time series forecasting\n",
    "\n",
    "- Temporal features: day, month, year, day of week, week of year, quarter etc.\n",
    "- Holiday flag\n",
    "- FFT features(as described above)\n",
    "- trend features: linear_trend, rolling mean\n",
    "\n",
    "#### Processor\n",
    "\n",
    "- Binary encoder for categorical features\n",
    "- One hot encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feats = ['style_color_group', 'style_group','transaction_date',\n",
    "       'level3', 'level4', 'units_sold',\n",
    "       'final_price', 'is_promotion']\n",
    "\n",
    "train = train[selected_feats]\n",
    "\n",
    "class FeatureProcessor(TransformerMixin,BaseEstimator,RegressorMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "    Feature Engineering class.\n",
    "\n",
    "    Args:\n",
    "        params: \n",
    "            X:style_color_group level transaction data\n",
    "\n",
    "    Returns:\n",
    "        a) style_color_group level Featurized(date and daily price) data\n",
    "    \"\"\"\n",
    "    \n",
    "    def transform(self,X):\n",
    "\n",
    "        '''Method for feature collection and aggregation'''\n",
    "\n",
    "        X = self.add_date_feats(X)\n",
    "        FFT = self.add_seasonal_features(X)\n",
    "        trend_MA = self.add_trend_MA_feats(X)\n",
    "        X = pd.concat([X.reset_index(drop=True),FFT.reset_index(drop=True),trend_MA.reset_index(drop=True)],axis=1).\\\n",
    "            drop(['transaction_Elapsed','transaction_Dayofyear'],axis=1)\n",
    "        X = self.binarize_cat(X)\n",
    "        X = self.ohe_cat(X)\n",
    "        return X\n",
    "\n",
    "    def add_date_feats(self,X):\n",
    "\n",
    "        '''Method to extract date features'''\n",
    "        \n",
    "        X['transaction_date'] = pd.to_datetime(X['transaction_date'])\n",
    "        X['holiday_flag'] = X['transaction_date'].apply(lambda x: 0 if holidays.DE().get(x) is None else 1)\n",
    "        date = X['transaction_date'] \n",
    "        X = add_datepart(X,'transaction_date')\n",
    "        X = pd.concat([X,date],axis=1)\n",
    "        return X\n",
    "    \n",
    "    def add_seasonal_features(self,X):\n",
    "\n",
    "        '''method to extract FFT(fast fourier transform) based seasonal features'''\n",
    "        \n",
    "        y = X[['units_sold','transaction_date']]\n",
    "        def fourier_features(index, freq, order):\n",
    "            time = np.arange(len(index), dtype=np.float32)\n",
    "            k = 2 * np.pi * (1 / freq) * time\n",
    "            features = {}\n",
    "            for i in range(1, order + 1):\n",
    "                features.update({\n",
    "                    f\"sin_{freq}_{i}\": np.sin(i * k),\n",
    "                    f\"cos_{freq}_{i}\": np.cos(i * k),\n",
    "                })\n",
    "            return pd.DataFrame(features, index=index)\n",
    "        #to capture quarterly seasonality\n",
    "        FFT = fourier_features(y, freq=4, order=4)\n",
    "        return FFT\n",
    "    \n",
    "    def add_trend_MA_feats(self,X):\n",
    "\n",
    "        '''method to extract FFT(fast fourier transform) based seasonal features'''\n",
    "\n",
    "        y = X[['units_sold','transaction_date']]\n",
    "        #trend feature\n",
    "        dp = DeterministicProcess(\n",
    "        index=y.index,  # dates from the training data\n",
    "        order=1,             # the time dummy (trend)\n",
    "        drop=True,           # drop terms if necessary to avoid collinearity\n",
    "    )\n",
    "\n",
    "        trend = dp.in_sample()\n",
    "        \n",
    "        #MA feature\n",
    "        moving_average = y.rolling(\n",
    "        window=52,       # 365-day window\n",
    "        center=True,      # puts the average at the center of the window\n",
    "        min_periods=26,  # choose about half the window size\n",
    "    ).mean()           \n",
    "        X = pd.concat([trend,moving_average],axis=1)\n",
    "        X.columns = ['trend','MA']\n",
    "        return X\n",
    "    \n",
    "    def binarize_cat(self,X):\n",
    "\n",
    "        '''Method for binary categorical features'''\n",
    "\n",
    "        bin_col = ['transaction_Is_month_end',\n",
    "       'transaction_Is_month_start', 'transaction_Is_quarter_end',\n",
    "       'transaction_Is_quarter_start', 'transaction_Is_year_end',\n",
    "       'transaction_Is_year_start']\n",
    "        bin = sklearn.preprocessing.LabelBinarizer()\n",
    "        X[bin_col] = bin.fit_transform(X[bin_col])\n",
    "        X['transaction_Year'] = np.where(X['transaction_Year'] == '2022',0,1)\n",
    "        return X\n",
    "\n",
    "    def ohe_cat(self,X):\n",
    "\n",
    "        '''Method for onehot encode of categorical features'''\n",
    "\n",
    "        cat_col = ['transaction_Month','transaction_Dayofweek']\n",
    "        ohe = sklearn.preprocessing.OneHotEncoder(sparse=False)\n",
    "        ohe.fit(X[cat_col])\n",
    "        x = ohe.transform(X[cat_col])\n",
    "        x = pd.DataFrame(x)\n",
    "        x.columns = ohe.get_feature_names_out()\n",
    "        X = pd.concat([X.drop(cat_col,axis=1).reset_index(drop=True),x],axis=1)  \n",
    "        return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustraion- feature Engg. pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "unique_style_group = train.style_group.unique()\n",
    "train_1 = train[train['style_group'] == unique_style_group[1]]\n",
    "fp = FeatureProcessor()\n",
    "df1 = fp.transform(train_1)\n",
    "display(df1.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_group\n",
    "adf_stat = {}\n",
    "for style_group in unique_style_group[0:5]:\n",
    "    print(style_group)\n",
    "    train_1 = df[df.style_group == style_group]\n",
    "    print(train_1.shape)\n",
    "    train_1 = fp.transform(train_1)[['transaction_date','units_sold']]\n",
    "    train_1 = train_1.set_index('transaction_date')\n",
    "    grpd_weekly = train_1.resample('W').sum()\n",
    "    result = adfuller(grpd_weekly.values.flatten())\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    \n",
    "    print(f'p-value: {result[1]}')\n",
    "    for key, value in result[4].items():\n",
    "        print('Critial Values:')\n",
    "        print(f'   {key}, {value}')\n",
    "    adf_stat[i] = {'pval':result[1],'adf_val':result[0],'critical_value':list(result[4].values())} \n",
    "display(adf_stat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For few products series is not stationary, so may have to make the time series closer to being stationary may have to deseasonalize and detrend during forecasting pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "### Framework\n",
    "- We will use sktime for time series forecasting with base estimator to convert time series to tabular data regression problem\n",
    "- sktime is a unified framework for machine learning with time series, which takes care of feature extraction based on windowing, so we do not need to do it manually calculate the lag features.So sktime extracts the lag features automatically based on the windowing strategy.\n",
    "\n",
    "\n",
    "- There are different Windowing(make_reduction) Strategy\n",
    "\n",
    "a) \"recursive“(Default, one at a time at forecasting horizon based on latest)\n",
    "\n",
    "b) \"multioutput“(Single estimator capable of handling multioutput targets is fitted to all the future steps in the forecasting horizon)\n",
    "\n",
    "c)\"direct“(separate forecaster is fitted for each step ahead of the forecasting horizon.)\n",
    "\n",
    "- Also with different window based cross-validation strategy\n",
    "\n",
    "\n",
    "### Design\n",
    " - Style-group specific Models- With this we have flexibility to roll up the forecast in level4 and level3\n",
    " - Unit of data- Daily level- With this we have flexibility to roll up the forecast in weekly, monthly, quarterly etc.\n",
    " - forecasting is for last 2 weeks(14 days) and model trained on rest of the data(for each product)\n",
    " - For quick implementation assume quarterly and additive quarterly seasonality and Polynomial trend of degree 1\n",
    " - I used TransformedTargetForecaster from sktime to build Forecasting pipeline using default XGBoost as regressor\n",
    " - To make things simple, I built seperate model(local model) for each Style-group instead of one global model.\n",
    " \n",
    " ### Assumption\n",
    "  - A) Daily final_price will be available during forecasting period\n",
    "  - B) Alternatively next day forecasting can be done based on last day price level\n",
    "\n",
    "### Performance Evaluation\n",
    "  - A) MAPE(Mean Absolute Percentage Error) generally is used as performance metric, however sMAPE is a better metric than MAPE to avoid dealing with an unbounded metric.\n",
    "  - B) sMAPE is symmetric, so it doesn't matter if the actuals are higher or lower than the forecast, the error is treated the same way.\n",
    "  - C) sMAPE and MAPE are scale independent, so it can be used to compare forecasts across different products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training and Evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models'\n",
    "class Model_train_prediction(TransformerMixin,RegressorMixin,BaseEstimator):\n",
    "\n",
    "    '''Class for building model and prediction including evaluation\n",
    "        Input: Product level Featurized data at daily level\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,window_len, test_size,path):\n",
    "\n",
    "       '''test_size- Period of Forecasting(2 weeks)\n",
    "          window_len- number of lags sliding window transformation  \n",
    "       ''' \n",
    "       self.test_size = test_size \n",
    "       self.window_len = window_len\n",
    "       self.path = path\n",
    "       if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "\n",
    "\n",
    "    def data_prep(self,data):\n",
    "\n",
    "        '''Method to create train and test data'''\n",
    "\n",
    "        X = data[x_feats]\n",
    "        Y = data[[target]]\n",
    "        train_y, test_y,train_x, test_x = temporal_train_test_split(y=Y,X=X,test_size=self.test_size)\n",
    "        return train_y, test_y,train_x, test_x\n",
    "\n",
    "    def model_train(self,train_x, train_y,test_y):\n",
    "\n",
    "        '''Method to train Model using grid search'''\n",
    "\n",
    "        fh = ForecastingHorizon(values=test_y.index,is_relative=False)\n",
    "\n",
    "        #estimator- XGBoost Regressor\n",
    "        regressor = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=300,\\\n",
    "                                      learning_rate=0.05, n_jobs=-1,random_state=42,\\\n",
    "                                        max_depth=4,subsample=0.8,colsample_bytree=0.6)\n",
    "        \n",
    "        #grid search parameters\n",
    "        param_grid = {'estimator__max_depth':[3,4],\n",
    "                      'window_length':[15,30],\n",
    "                      'estimator__learning_rate':[0.05,0.1],\n",
    "                      'estimator__subsample':[0.7,0.8],\n",
    "                      'estimator__colsample_bytree':[0.5,0.6,0.7]}\n",
    "        \n",
    "        #Forecasting pipeline with deseasonalizer, detrender and regressor \n",
    "        #this window_len defines the lagged unit_sold feature the forecaster will extract\n",
    "        forecaster = TransformedTargetForecaster([\n",
    "        (\"deseasonalize\",Deseasonalizer(model=\"additive\",sp = 90)),\n",
    "        (\"dtrend\",Detrender(forecaster = PolynomialTrendForecaster(degree=1))),\n",
    "        (\"forecast\", MultioutputTabularRegressionForecaster(estimator=regressor,window_length=self.window_len)),\n",
    "    ])\n",
    "        forecaster = MultioutputTabularRegressionForecaster(estimator=regressor,window_length=self.window_len)\n",
    "\n",
    "        #fitting with grid search\n",
    "        cv = SlidingWindowSplitter(initial_window=30, window_length=15)\n",
    "        forecaster = ForecastingRandomizedSearchCV(forecaster,\\\n",
    "                                             n_jobs=-1,verbose=1,n_iter=5,param_distributions=param_grid,cv=cv)\n",
    "        forecaster.fit(y=train_y,X=train_x,fh = fh)\n",
    "        return forecaster,fh\n",
    "\n",
    "    def make_forecast(self, forecaster, test_x, fh):\n",
    "\n",
    "        \"\"\"Method for Forecasting and make negative values to zero\"\"\"\n",
    "\n",
    "        pred_y = round(forecaster.predict(X=test_x, fh=fh), 0)\n",
    "        pred_y = np.where(pred_y <= 0, 0, pred_y)\n",
    "        pred_y = pd.DataFrame(pred_y)\n",
    "        pred_y.columns = ['units_sold']\n",
    "        return pred_y\n",
    "\n",
    "    def evaluate(self,test_y,pred_y):\n",
    "\n",
    "        '''Method to evaluate Forecasting result'''\n",
    "\n",
    "        def MAPE(Y_actual,Y_Predicted):\n",
    "\n",
    "            \"\"\"Mean Absolute Percentage Error with 0.1 added to denominator to avoid division by zero\"\"\"\n",
    "            mape = np.mean(np.abs((Y_actual - Y_Predicted)/(Y_actual + 0.1)))*100\n",
    "            return mape[0]\n",
    "\n",
    "        def calculate_smape(actual, predicted) -> float: \n",
    "\n",
    "            \"\"\"Symmetric mean absolute percentage error with 0.1 added to denominator to avoid division by zero\"\"\"\n",
    "  \n",
    "            return round( \n",
    "                np.mean( \n",
    "                    np.abs(predicted - actual) / \n",
    "                    ((np.abs(predicted) + np.abs(actual) + 0.1)/2) \n",
    "                )*100, 2,\n",
    "            )[0] \n",
    "        \n",
    "        mape = MAPE(test_y,pred_y)\n",
    "        mae = sklearn.metrics.mean_absolute_error(test_y,pred_y)\n",
    "        smape = calculate_smape(test_y,pred_y)\n",
    "        return mape,mae,smape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance and analysis- we can have an estimation by fitting the estimator\n",
    "\n",
    "- We start with all features and with insight of feature importance we can have a better understanding of feature selection\n",
    "- As I build local model for each style_group, I don't need to use 'month_of_launch','season_start_date', 'season_end_date','current_season', 'initial_season' as they are style_group specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feats = ['final_price', 'is_promotion', 'holiday_flag', 'transaction_Year',\n",
    "       'transaction_Week', 'transaction_Day', 'transaction_Is_month_end',\n",
    "       'transaction_Is_month_start', 'transaction_Is_quarter_end',\n",
    "       'transaction_Is_quarter_start', 'transaction_Is_year_end',\n",
    "       'transaction_Is_year_start','sin_4_1', 'cos_4_1',\n",
    "       'sin_4_2', 'cos_4_2', 'sin_4_3', 'cos_4_3', 'sin_4_4', 'cos_4_4',\n",
    "       'trend', 'MA', 'transaction_Month_1', 'transaction_Month_2',\n",
    "       'transaction_Month_3', 'transaction_Month_4', 'transaction_Month_5',\n",
    "       'transaction_Month_6', 'transaction_Month_7', 'transaction_Month_8',\n",
    "       'transaction_Month_9', 'transaction_Month_10', 'transaction_Month_11',\n",
    "       'transaction_Month_12', 'transaction_Dayofweek_0',\n",
    "       'transaction_Dayofweek_1', 'transaction_Dayofweek_2',\n",
    "       'transaction_Dayofweek_3', 'transaction_Dayofweek_4',\n",
    "       'transaction_Dayofweek_5', 'transaction_Dayofweek_6']\n",
    "target = 'units_sold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models'\n",
    "cols_to_drop = ['sales_price','style_color_group', 'style_group', 'level3', 'level4', 'units_sold','transaction_date']\n",
    "target = 'units_sold'\n",
    "cols = list(df.columns)\n",
    "x_feats = [x for x in df1.columns if x not in cols_to_drop]\n",
    "mtp = Model_train_prediction(window_len=30, test_size=14,path = path)\n",
    "train_y, test_y,train_x, test_x = mtp.data_prep(df1)\n",
    "forecaster,fh = mtp.model_train(train_x, train_y,test_y)\n",
    "\n",
    "#fitting the estimator\n",
    "forecaster.estimator.fit(train_x,train_y)\n",
    "\n",
    "#Grid search best parameters\n",
    "print('Optimized Parameters')\n",
    "print(forecaster.estimator_.get_params())\n",
    "print('Optimized window length')\n",
    "print(forecaster.window_length_)\n",
    "\n",
    "#feature importance plot and table.make a figsize of 10,10\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "importance = forecaster.estimator.feature_importances_\n",
    "indices = np.argsort(importance)\n",
    "features = train_x.columns\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importance[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance');\n",
    "\n",
    "#also create a table of feature importance with names and values in descending order\n",
    "feature_importance = pd.DataFrame({'feature':train_x.columns,'importance':forecaster.estimator.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n",
    "feature_importance['cum_importance'] = feature_importance['importance'].cumsum()\n",
    "display(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance & Selection\n",
    "\n",
    "- As expected moving average of sales,promotion,trend, price and a few temporal features emerged as important features\n",
    "- We select features which constitute  > 98% of the variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the least important features have values cum_importance > 0.98\n",
    "\n",
    "drop_cols = list(feature_importance[feature_importance['cum_importance'] > 0.98]['feature'])\n",
    "x_feats = [x for x in x_feats if x not in drop_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orchestrating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '../models'\n",
    "evaluation_metric = {}\n",
    "prediction = {}\n",
    "actual = {}\n",
    "if __name__ == '__main__':\n",
    "    mtp = Model_train_prediction(window_len=30, test_size=14,path = path)\n",
    "    for i in unique_style_group:\n",
    "        fp = FeatureProcessor()\n",
    "        train_feat = fp.transform(df[df.style_group==i])\n",
    "        train_y, test_y,train_x, test_x = mtp.data_prep(train_feat)\n",
    "        forecaster,fh = mtp.model_train(train_x, train_y,test_y)\n",
    "        with open(os.path.join(path, f'item_{i}.pkl'),'wb') as f:\n",
    "            pickle.dump(forecaster,f)\n",
    "        pred_y = mtp.make_forecast(forecaster,test_x,fh)\n",
    "        prediction[i] = list(pred_y.units_sold.values)\n",
    "        actual[i] = list(test_y.units_sold.values)\n",
    "        evaluation_metric[i] = mtp.evaluate(test_y.reset_index(drop=True),pred_y.reset_index(drop=True))\n",
    "    actuals = pd.DataFrame(actual)\n",
    "    actuals.columns = [f'actual_{i}' for i in actual.keys()]\n",
    "    predictions = pd.DataFrame(prediction)\n",
    "    predictions.columns = [f'prediction_{i}' for i in prediction.keys()]\n",
    "    acutal_prediction = pd.concat([actuals,predictions],axis=1)\n",
    "    acutal_prediction.to_csv(os.path.join(path, 'acutal_prediction.csv'),index=False) \n",
    "    acutal_prediction.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcasting Performance- Evaluation(Style-Group Hierarchy level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Qualitative analysis- Actual vs. Forecast Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_style_group:\n",
    "    fp = FeatureProcessor()\n",
    "    train_feat = fp.transform(df[df.style_group==i])\n",
    "    train_y, test_y,train_x, test_x = mtp.data_prep(train_feat)\n",
    "    pred_y = pd.DataFrame(prediction[i])\n",
    "    pred_y = pred_y.set_index(test_y.index)\n",
    "    plot_series(train_y[-30:], test_y, pred_y, labels = ['train_y','test_y','pred_y'],x_label = 'date',y_label = 'sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## style_group level evaluation metric-daily\n",
    "\n",
    "#make pandas dataframe of evaluation_metric\n",
    "\n",
    "evaluation_metric = pd.DataFrame(evaluation_metric).T\n",
    "evaluation_metric.columns = ['mape','mae','smape']\n",
    "evaluation_metric['style_group'] = evaluation_metric.index\n",
    "\n",
    "evaluation_metric = evaluation_metric.reset_index(drop=True)\n",
    "\n",
    "#make style_group as first column\n",
    "cols = list(evaluation_metric.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "evaluation_metric = evaluation_metric[cols]\n",
    "evaluation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actual vs. Forecasted mean accross style_group\n",
    "\n",
    "mean_forecast_vs_actual = {}\n",
    "for i in unique_style_group:\n",
    "    pred_y = prediction[i]\n",
    "    test_y = actual[i]\n",
    "    mean_forecast = np.mean(pred_y)\n",
    "    mean_actual = np.mean(test_y)\n",
    "    mean_forecast_vs_actual[i] = [mean_forecast, mean_actual]\n",
    "mean_forecast_vs_actual = pd.DataFrame(mean_forecast_vs_actual).T\n",
    "mean_forecast_vs_actual.columns = ['mean_forecast','mean_actual']\n",
    "mean_forecast_vs_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Forecasting\n",
    "\n",
    "##### We may roll it up to the level4 hierarchy and see the performance\n",
    "\n",
    "##### Also we can aggregate the forecasted sales to weekly level and analyse the performance\n",
    "\n",
    "##### Before that it's better to build the Inference pipeline to forecast for the next 14 days for all style_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Pipeline\n",
    "\n",
    "- Preparing data to forecast for next 14 days for all style_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_y_df = pd.DataFrame()\n",
    "for i in unique_style_group:\n",
    "    fp = FeatureProcessor()\n",
    "    train_feat = fp.transform(df[df.style_group==i])\n",
    "    train_y, test_y,train_x, test_x = mtp.data_prep(train_feat)\n",
    "    test_y['style_group'] = i\n",
    "    test_x['style_group'] = i\n",
    "    test_y['day'] = range(1,15)\n",
    "    test_df = test_df.append(test_x)\n",
    "    test_y_df = test_y_df.append(test_y)\n",
    "test_df.to_csv('../data/test_data.csv',index=False) \n",
    "test_y_df.to_csv('../data/test_data_y.csv',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Inference(TransformerMixin,RegressorMixin,BaseEstimator):\n",
    "\n",
    "    '''Class for using model for prediction\n",
    "        Input: Featurized data at daily level\n",
    "    '''\n",
    "    def __init__(self,model_path):\n",
    "\n",
    "       self.model_path = model_path \n",
    "\n",
    "    def collect_data_model(self,data):\n",
    "        test_x = data.drop('style_group',axis=1)\n",
    "        with open(self.model_path ,'rb') as fout:\n",
    "            model= pickle.load(fout)\n",
    "        return model,test_x    \n",
    "\n",
    "    def make_forecast(self,model,test_x):\n",
    "\n",
    "        '''Method for Forecasting'''\n",
    "\n",
    "        pred_y = round(model.predict(fh = model.fh,X=test_x),0)\n",
    "        pred_y = np.where(pred_y <= 0, 0, pred_y)\n",
    "        pred_y = pd.DataFrame(pred_y)\n",
    "        pred_y.columns = ['units_sold']\n",
    "        return pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../forecast/'\n",
    "model_path = '../models/'\n",
    "predictions = pd.DataFrame()\n",
    "if __name__ == '__main__':\n",
    "    test_df = pd.read_csv(os.path.join('../data','test_data.csv'))\n",
    "    for i in unique_style_group:\n",
    "        test_x = test_df[test_df.style_group == i]\n",
    "        prod_mod_path = os.path.join(model_path,f'item_{i}.pkl')\n",
    "        mi = Model_Inference(model_path=prod_mod_path)\n",
    "        model,test_x = mi.collect_data_model(test_x)\n",
    "        pred = mi.make_forecast(model,test_x)\n",
    "        pred = pd.DataFrame({'style_group':i,'forecast':pred.units_sold.values, 'day':list(range(1,15))})\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        pred.to_csv(os.path.join(path,f'item_{i}.csv'),index=False)\n",
    "        predictions = predictions.append(pred)\n",
    "    predictions.to_csv(os.path.join(path,'all_style_prediction.csv'),index=False) \n",
    "\n",
    "predictions = pd.merge(test_y_df,predictions, on=['style_group','day'], how='left')\n",
    "predictions.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Roll up prediction to level4\n",
    "\n",
    "# predictions = pd.merge(predictions, train[['style_group','level3','level4']].drop_duplicates(), on='style_group', how='left')\n",
    "predictions['abs_error'] = abs(predictions['units_sold'] - predictions['forecast'])\n",
    "\n",
    "#adding a small value to denominator to avoid division by zero\n",
    "predictions['abs_percentage_error'] = abs(predictions['units_sold'] - predictions['forecast'])/(predictions['units_sold'] + 0.2)\n",
    "display(predictions.head())\n",
    "print('overall MAPE at style_group level',np.mean(predictions['abs_percentage_error'])*100)\n",
    "\n",
    "#actual vs. forecasted in level3\n",
    "\n",
    "predictions_level3 = predictions.groupby(['level3','day'],as_index=False).agg({'units_sold':'sum','forecast':'sum'})\n",
    "predictions_level3.columns = ['level3','day','actual','forecast']\n",
    "predictions_level3['abs_error'] = abs(predictions_level3['actual'] - predictions_level3['forecast'])\n",
    "predictions_level3['abs_percentage_error'] = abs(predictions_level3['actual'] - predictions_level3['forecast'])/predictions_level3['actual']\n",
    "\n",
    "predictions_level3.head()\n",
    "print('overall MAPE at level4',np.mean(predictions_level3['abs_percentage_error'])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level3 wise predicted vs. actual sales plt day wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_level_3 = train['level3'].unique()\n",
    "for i in unique_level_3:\n",
    "    pred = predictions_level3[predictions_level3.level3 == i]   \n",
    "    plot_series(pred.actual, pred.forecast, labels = ['actual','forecast'],x_label = 'day',y_label = 'sales',title = f'{i}')  \n",
    "\n",
    "# roll it upto weekly level\n",
    "predictions_level3['week'] = np.where(predictions_level3['day'] <= 7, 1,2)\n",
    "predictions_level3_week = predictions_level3.groupby(['level3','week'],as_index=False).agg({'actual':'sum','forecast':'sum'})\n",
    "predictions_level3_week.columns = ['level3','week','actual','forecast']\n",
    "predictions_level3_week['abs_error'] = abs(predictions_level3_week['actual'] - predictions_level3_week['forecast'])\n",
    "predictions_level3_week['abs_percentage_error'] = abs(predictions_level3_week['actual'] - predictions_level3_week['forecast'])/predictions_level3_week['actual']\n",
    "predictions_level3_week.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement and Future work(with more time)\n",
    "\n",
    "- A) Other base estimators like LightGBM, CatBoost, Random Forest, Linear Regression can be used to build the forecasting pipeline\n",
    "- B) We can use different windowing strategy and cross-validation strategy,seasonality and trend\n",
    "- C) A global model can be built instead of local model for each style_group to capture the intaction between style_group features(price, unit_sold etc.)\n",
    "\n",
    "  a) Darts: Perhaps the most popular with almost all forecasting algorithms including cutting-edge DL(BlockRNNModel,NBEATSModel,NHiTSModel,TCNModel etc.)and transformer(TransformerModel,TFTModel etc.) based models.\n",
    "\n",
    "  b) Nixtla(mlforecast): Global ML-based forecasting algorithms like LinearRegression, LightGBM or XGBoost, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Elasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship with price and sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df.groupby('style_group').agg({'final_price':['max','min','std','mean']})\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "unique_style_group = df.style_group.unique()\n",
    "price = df.groupby(['style_group','transaction_date'],as_index=False).agg({'final_price':['mean'], 'units_sold':['sum']})\n",
    "price.columns = ['_'.join(x) for x in price.columns.ravel()]\n",
    "for i in unique_style_group:\n",
    "    temp = price[price.product_code_ == i][['final_price_mean','units_sold_sum']]\n",
    "# df_price_cross = df_price_cross.rename(columns = style)\n",
    "    fig.set_size_inches(5,3)\n",
    "    sns.regplot(x=temp.final_price_mean, y=temp.units_sold_sum, line_kws={\"color\":\"r\",\"alpha\":0.7,\"lw\":5})\n",
    "    plt.title(f'product_code {i}')\n",
    "    plt.xlabel(\"daily_price\")\n",
    "    plt.ylabel(\"sales\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This plots show there is inverse relationship between price and sales except for style group f551448b750f6b19c9370836b8f0d1f2e571db67014755e544ce2b68278529e4\n",
    "- These negative relationship(Price elasticities of demand) are intuitive as price and quantity demanded mostly move in opposite directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "- I use here simple beta regression model to estimate the price elasticity of demand for different hierarchy level(style_group,level4,level3 etc.)\n",
    "- I tried to estimate both price elasticity and cross price elasticity of demand.\n",
    "- Disclaimer- Due to time contraint,I merely implement the concept with out much rigor to have an accurate estimation.\n",
    "\n",
    "\n",
    "\n",
    "#### Data preparation\n",
    "\n",
    "- Based on the above, we need to transform the data from long to wide format to have price of all style_group as features and sales as target variable for each date\n",
    "- Then for each style_group we can fit a beta regression model to estimate the price elasticity and cross price elasticity of demand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('../data/transaction_grid.csv')\n",
    "df['l3_l4'] = df['level3'] + '_' + df['level4']\n",
    "df['holiday_flag'] = df['transaction_date'].apply(lambda x: 0 if holidays.DE().get(x) is None else 1)\n",
    "df['month'] = df['transaction_date'].dt.month\n",
    "price_sensitivity_holiday = pd.DataFrame(df.groupby(['l3_l4','holiday_flag']).agg({'units_sold':['max','min','mean']\\\n",
    "    ,'final_price':['mean'],'season_start_date':['max'],'season_end_date':['max']})).reset_index()\n",
    "price_sensitivity_holiday   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['holiday_flag'] = df['transaction_date'].apply(lambda x: 0 if holidays.DE().get(x) is None else 1)\n",
    "#convert to month name\n",
    "import calendar\n",
    "df['month'] = df['transaction_date'].dt.month\n",
    "price_sensitivity_month = pd.DataFrame(df.groupby(['l3_l4','month']).agg({'units_sold':['max','min','mean']\\\n",
    "    ,'final_price':['mean']})).reset_index()\n",
    "price_sensitivity_month.columns = ['_'.join(x) for x in price_sensitivity_month.columns.ravel()]\n",
    "#sort it with l3_l4 and month\n",
    "price_sensitivity_month = price_sensitivity_month.sort_values(by=['l3_l4_','month_'])\n",
    "price_sensitivity_month['month_'] = price_sensitivity_month['month_'].apply(lambda x: calendar.month_abbr[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_l3_l4:\n",
    "    temp = price_sensitivity_month[price_sensitivity_month.l3_l4_ == i][['final_price_mean','month_','units_sold_mean','units_sold_max', 'units_sold_min']]\n",
    "    #make price bin with interval of 1 and group by mean, min and max\n",
    "\n",
    "    fig.set_size_inches(10,3)\n",
    "    plt.plot(temp.month_,temp.units_sold_max,'r',label='max_sales')\n",
    "    plt.plot(temp.month_,temp.units_sold_min,'b',label='min_sales')\n",
    "    plt.plot(temp.month_,temp.units_sold_mean,'g',label='mean_sales')\n",
    "    plt.legend()\n",
    "    #second y axis for price\n",
    "    ax2 = plt.twinx()\n",
    "    ax2.plot(temp.month_,temp.final_price_mean,'y',label='mean_price')\n",
    "    \n",
    "    plt.legend()\n",
    "    #regression plot with confidence interval    \n",
    "    plt.title(f'product_code {i}')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"month\")\n",
    "    #remove negative from y axis\n",
    "    plt.ylim(bottom=0)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_sensitivity['revenue'] = price_sensitivity['final_price_round_'] * price_sensitivity['units_sold_mean']\n",
    "for i in unique_l3_l4:\n",
    "    temp = price_sensitivity[price_sensitivity.l3_l4_ == i][['final_price_round_','units_sold_max','units_sold_min','units_sold_mean','revenue']]\n",
    "    #make price bin with interval of 1 and group by mean, min and max\n",
    "\n",
    "    fig.set_size_inches(10,3)\n",
    "    plt.plot(temp.final_price_round_,temp.units_sold_mean,'g',label='mean_sales')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"daily_price\")\n",
    "    ax2 = plt.twinx()\n",
    "    ax2.plot(temp.final_price_round_,temp.revenue,'b',label='mean_revenue')\n",
    "\n",
    "    #regression plot with confidence interval\n",
    "    sns.regplot(x=temp.final_price_round_, y=temp.revenue, line_kws={\"color\":\"y\",\"alpha\":0.5,\"lw\":2},ci=95,\\\n",
    "        scatter_kws={\"color\":\"b\",\"alpha\":0.9,\"s\":100},marker='.')    \n",
    "    plt.title(f'product_code {i}')\n",
    "    #put the legend in bottom right\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    #remove negative from y axis\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.show()\n",
    "    #print the correlation coefficient\n",
    "    cor = round(np.corrcoef(temp.final_price_round_,temp.units_sold_mean)[0][1] * 100,2)\n",
    "    #print the correlation coefficient red if negative and green if positive\n",
    "    if cor < 0:\n",
    "        print('')\n",
    "        print('relationship with price:',colored(cor,'green'))\n",
    "    else:\n",
    "        print('relationship with price:',colored(cor,'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price and revenue moves in same direction\n",
    "#### so the price maximising the sales is same as price maximising the revenue\n",
    "#### But it should depend on the month/season as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonal price recommendation\n",
    "\n",
    "- Dynamic pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trousers_long = price_sensitivity_month[price_sensitivity_month.l3_l4_ == 'Trousers_long'][['final_price_mean','month_','units_sold_mean','units_sold_max', 'units_sold_min','revenue']]\n",
    "unique_month = trousers_long.month_.unique()\n",
    "\n",
    "#make it season with grouping months into season\n",
    "season = {'spring':['Mar','Apr','May'],'summer':['Jun','Jul','Aug'],'autumn':['Sep','Oct','Nov'],'winter':['Dec','Jan','Feb']}\n",
    "#apply this season to month column\n",
    "trousers_long['season'] = trousers_long['month_'].apply(lambda x: [k for k,v in season.items() if x in v][0])\n",
    "unique_season = trousers_long.season.unique()\n",
    "price_sensitivity_month['revenue'] = price_sensitivity_month['final_price_mean'] * price_sensitivity_month['units_sold_mean']\n",
    "for i in unique_season:\n",
    "    temp = trousers_long[trousers_long.season == i][['revenue',\n",
    "       'units_sold_mean', 'final_price_mean']]\n",
    "    #make price bin with interval of 1 and group by mean, min and max\n",
    "\n",
    "    fig.set_size_inches(10,3)\n",
    "    plt.plot(temp.final_price_mean,temp.units_sold_mean,'g',label='mean_sales')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"price\")\n",
    "    ax2 = plt.twinx()\n",
    "    ax2.plot(temp.final_price_mean,temp.revenue,'b',label='mean_revenue')\n",
    "\n",
    "    #regression plot with confidence interval\n",
    "    sns.regplot(x=temp.final_price_mean, y=temp.revenue, line_kws={\"color\":\"y\",\"alpha\":0.5,\"lw\":2},ci=95,\\\n",
    "        scatter_kws={\"color\":\"b\",\"alpha\":0.9,\"s\":100},marker='.')    \n",
    "    plt.title(f'season {i}')\n",
    "    #put the legend in bottom right\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    #remove negative from y axis\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.show()\n",
    "    #print the correlation coefficient\n",
    "    cor = round(np.corrcoef(temp.final_price_mean,temp.units_sold_mean)[0][1] * 100,2)\n",
    "    #print the correlation coefficient red if negative and green if positive\n",
    "    if cor < 0:\n",
    "        print('')\n",
    "        print('relationship with price:',colored(cor,'green'))\n",
    "    else:\n",
    "        print('relationship with price:',colored(cor,'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "df['final_price_round'] = np.round(df['final_price']).astype(int)\n",
    "price_sensitivity = pd.DataFrame(df.groupby(['l3_l4','final_price_round']).agg({'units_sold':['max','min','std','mean']})).reset_index()\n",
    "price_sensitivity = price_sensitivity[(price_sensitivity['units_sold']['std'] > 0)]\n",
    "price_sensitivity.columns = ['_'.join(x) for x in price_sensitivity.columns.ravel()]\n",
    "unique_style_group = df.style_group.unique()\n",
    "unique_l3_l4 = df.l3_l4.unique()\n",
    "for i in unique_l3_l4:\n",
    "    temp = price_sensitivity[price_sensitivity.l3_l4_ == i][['final_price_round_','units_sold_max','units_sold_min','units_sold_mean']]\n",
    "    #make price bin with interval of 1 and group by mean, min and max\n",
    "\n",
    "    fig.set_size_inches(10,3)\n",
    "    plt.plot(temp.final_price_round_,temp.units_sold_max,'r',label='max')\n",
    "    plt.plot(temp.final_price_round_,temp.units_sold_min,'b',label='min')\n",
    "    plt.plot(temp.final_price_round_,temp.units_sold_mean,'g',label='mean')\n",
    "\n",
    "    #regression plot with confidence interval\n",
    "    sns.regplot(x=temp.final_price_round_, y=temp.units_sold_mean, line_kws={\"color\":\"y\",\"alpha\":0.5,\"lw\":2},ci=95,\\\n",
    "        scatter_kws={\"color\":\"b\",\"alpha\":0.9,\"s\":100},marker='.')    \n",
    "    plt.title(f'product_code {i}')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"daily_price\")\n",
    "    #remove negative from y axis\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.ylabel(\"sales\")\n",
    "    plt.show()\n",
    "    #print the correlation coefficient\n",
    "    cor = round(np.corrcoef(temp.final_price_round_,temp.units_sold_mean)[0][1] * 100,2)\n",
    "    #print the correlation coefficient red if negative and green if positive\n",
    "    if cor < 0:\n",
    "        print('')\n",
    "        print('relationship with price:',colored(cor,'green'))\n",
    "    else:\n",
    "        print('relationship with price:',colored(cor,'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### style_group level   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to avoid long style_group names\n",
    "style = {}\n",
    "for i,style_group in enumerate(unique_style_group):\n",
    "    style[style_group] = f'style_{i+1}'\n",
    "\n",
    "df_price = df[['style_group', 'transaction_date','sales_price','units_sold','level3','level4']]\n",
    "df_price = df_price['style_group'].map(style).to_frame().join(df_price.drop('style_group', axis=1))\n",
    "df_price['l3_l4'] = df_price['level3'] + '_' + df_price['level4']\n",
    "df_price_cross = df_price.pivot(index='transaction_date', columns='style_group', values=['sales_price','units_sold'])\n",
    "\n",
    "df_price_cross.columns = ['_'.join(x) for x in df_price_cross.columns.ravel()]\n",
    "df_price_cross.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Individual OLS model to estimate price elasticity and cross-price elasticity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "for i in list(style.values()):\n",
    "    #keep all price related columns\n",
    "    print(f'for style_group {i}')\n",
    "    style_df = pd.concat([df_price_cross.iloc[:,0:8], df_price_cross[f'units_sold_{i}']], axis=1).dropna()\n",
    "    model = sm.OLS(style_df[f'units_sold_{i}'],style_df.drop([f'units_sold_{i}'],axis=1),missing='drop')\n",
    "    result = model.fit()\n",
    "    results_summary = result.summary()\n",
    "    display(results_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These co-efficients provides us the price and cross price elasticity for each style_group\n",
    "\n",
    "- For most of the style group we see negative price elasticity(co-efficient value own style) which is expected\n",
    "- For cross price elasticity we see mix of positive values and negative values which is expected as some style_group are complements and some are substitutes\n",
    "- Based on these co-efficients we can also simulate/estimate unit_sold(sales) for a given price level for each style_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can have the estimate the elasticity at level4 level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "\n",
    "df_price = df[['style_group', 'transaction_date','final_price','units_sold','level3','level4']]\n",
    "# df_price = df_price['style_group'].map(style).to_frame().join(df_price.drop('style_group', axis=1))\n",
    "df_price['l3_l4'] = df_price['level3'] + '_' + df_price['level4']\n",
    "df_price = df_price[['l3_l4','transaction_date','final_price','units_sold']].groupby(['l3_l4','transaction_date'],as_index=False).agg({'final_price':'mean','units_sold':'sum'})\n",
    "df_price_cross = df_price.pivot(index='transaction_date', columns='l3_l4', values=['final_price','units_sold'])\n",
    "\n",
    "df_price_cross.columns = ['_'.join(x) for x in df_price_cross.columns.ravel()]\n",
    "df_price_cross.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_l3_l4 = df_price['l3_l4'].unique()\n",
    "for i in unique_l3_l4:\n",
    "    #keep all price related columns\n",
    "    print(f'for style_group {i}')\n",
    "    style_df = pd.concat([df_price_cross.iloc[:,0:4], df_price_cross[f'units_sold_{i}']], axis=1)\n",
    "    model = sm.OLS(np.log1p(style_df[f'units_sold_{i}']),style_df.drop([f'units_sold_{i}'],axis=1),missing='drop')\n",
    "    result = model.fit()\n",
    "    results_summary = result.summary()\n",
    "    display(results_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### While most of the products at level_4 are having negative elasticity, only 'final_price_Trousers_short' is the exception with both elasticity and cross-price elasticity is positive.\n",
    "\n",
    "- A possible remidy for this to conduct a Bayesian analysis to estimate the price elasticity and cross-price elasticity with prior distribution(eg. Half-Normal distribution) of the co-efficients\n",
    "\n",
    "- From this co-efficient we can infer the substitutability or complementarity of the products.eg. Trousers_long and short-sleeve_tshirt are complements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement and Future work(with more time)\n",
    "\n",
    "- A) As it's time series other features to be considered like seasonality, trend, promotion etc along with price features.\n",
    "- B) To estimate the more correct estimation we need to detrend and deseasonalize the data before fitting the model.\n",
    "- C) Bayesian regression with informed priors distribution of the co-efficients and to have more accurate and intuitive estimation of the price elasticity and cross-price elasticity.\n",
    "- D) Recommndation of optimum price level on daily/weekly basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
